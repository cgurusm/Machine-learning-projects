{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Back propagation in tensorflow\n",
    "Tensorflow can keep track of our operations with the aid of its computation graph and then automatically update the model's variables based on a system called back propagation.\n",
    "> We normally used backpropagation to minimize our loss function with respect to the models variables or parameters.\n",
    "This minimization is done via declaring an optimization function. Once we have declared an optimization function, Tensorflow will go through and automatically figure out the backpropagation values for all our computation on the graph and then it automatically modify or update the variables in the graph accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#start a session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple regression algorithm \n",
    "In this example we will sample random numbers from a normal distribution with mean 1 and standard deviation 0.1. Then we will apply a simple linear operation on it, i.e multiply it by a variable $W$, then we would apply a loss function (L2 norm) between the output and the target which for this example we will set to a constant 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our data, placeholders and variables\n",
    "x_val = np.random.normal(1, 0.1, 100)   #input values\n",
    "y_val = np.repeat(10., 100)   #target values\n",
    "\n",
    "# placeholders\n",
    "x_data = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "\n",
    "#Variables\n",
    "w = tf.Variable(tf.random_normal(shape=[1]))\n",
    "\n",
    "# Add linear function to the computational graph\n",
    "y_pred = tf.multiply(x_data, w)\n",
    "\n",
    "# Add a loss function(L2 norm)\n",
    "loss = tf.square(y_pred - y_target)\n",
    "\n",
    "# initialize our variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Most optimization algorithms need\n",
    "to know how far to step in each iteration. This distance is controlled\n",
    "by the learning rate. If our learning rate is too big, our algorithm might\n",
    "overshoot the minimum, but if our learning rate is too small, out\n",
    "algorithm might take too long to converge; this is related to the\n",
    "vanishing and exploding gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare an optimizer: here i use gradient descent\n",
    "my_opt = tf.train.GradientDescentOptimizer(learning_rate=0.02)\n",
    "\n",
    "#Create the train step\n",
    "train_step = my_opt.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we train the model by looping through the algorithm a number of times. We will print the updated result for every 20th iteration. To train, we will use a stochastic gradient descent training by picking a value from the input and target at random, and feeding it to the graph.\n",
    "\n",
    ">Tensorflow will automatically compute the loss and update the variable W to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #20 W = [5.460243]\n",
      "Loss = [18.638432]\n",
      "Step #40 W = [8.000424]\n",
      "Loss = [6.931777]\n",
      "Step #60 W = [9.075418]\n",
      "Loss = [0.8593326]\n",
      "Step #80 W = [9.735153]\n",
      "Loss = [0.7545254]\n",
      "Step #100 W = [9.869209]\n",
      "Loss = [0.02213772]\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 100\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = [x_val[rand_index]]\n",
    "    rand_y = [y_val[rand_index]]\n",
    "    #Run the graph\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    #Print the result on every 20th iteration\n",
    "    if(i + 1) % 20 == 0:\n",
    "        print('Step #' + str(i+1) + ' W = ' + str(sess.run(w)))\n",
    "        print('Loss = ' + str(sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another sample for a binary classification problem\n",
    "In this example, we will create two target classes ```Y(1, 0 )``` from two different normal distribution $N(-1,1)$ and $N(3,1)$. We will also generate target placeholder for the data and the weight label W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph to re-initialize the session\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data\n",
    "x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(3, 1, 50)))\n",
    "y_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))\n",
    "\n",
    "#create placeholders\n",
    "x_data = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "\n",
    "#Variable\n",
    "w = tf.Variable(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We initialize w to be around a mean of 10 just to show how the algorithm converges from a far off value\n",
    "\n",
    "Next we create the function. Since this is a binary classification and the predictions are probabilities between 0 and 1, we apply the sigmoid function to the linear function above. \n",
    ">This is automatically done by the Tensorflow function ```tf.nn.sigmoid_cross_entropy_with_logits```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function\n",
    "y_pred = tf.add(x_data, w)\n",
    "\n",
    "# initialize our variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function: Sigmoid cross entropy loss\n",
    "x_entropy_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_target, logits=y_pred)\n",
    "\n",
    "# Declare an optimizer: here i use gradient descent\n",
    "my_opt = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "my_opt2 = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "#Create the train step\n",
    "train_step = my_opt.minimize(x_entropy_loss)\n",
    "train_step2 = my_opt2.minimize(x_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Finally, we loop through a randomly selected data point several\n",
    "hundred times and update the variable W accordingly. Every 200\n",
    "iterations, we will print out the value of w and the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #200 W = -0.8044722\n",
      "Loss = [0.11743699]\n",
      "Step #400 W = -0.94036\n",
      "Loss = [0.08700228]\n",
      "Step #600 W = -0.786594\n",
      "Loss = [0.17700219]\n",
      "Step #800 W = -0.90714014\n",
      "Loss = [0.15840782]\n",
      "Step #1000 W = -0.88285613\n",
      "Loss = [0.41357884]\n",
      "Step #1200 W = -1.0001022\n",
      "Loss = [0.12860726]\n",
      "Step #1400 W = -0.870744\n",
      "Loss = [0.14574195]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1400):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = [x_vals[rand_index]]\n",
    "    rand_y = [y_vals[rand_index]]\n",
    "    #Run the graph\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    #Print the result on every 20th iteration\n",
    "    if(i + 1) % 200 == 0:\n",
    "        print('Step #' + str(i+1) + ' W = ' + str(sess.run(w)))\n",
    "        print('Loss = ' + str(sess.run(x_entropy_loss, feed_dict={x_data: rand_x, y_target: rand_y})))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #200 W = -0.8709925\n",
      "Loss = [0.1203137]\n",
      "Step #400 W = -0.8737013\n",
      "Loss = [0.28539962]\n",
      "Step #600 W = -0.8747969\n",
      "Loss = [0.23416257]\n",
      "Step #800 W = -0.8769944\n",
      "Loss = [0.10669471]\n",
      "Step #1000 W = -0.8776215\n",
      "Loss = [0.08085661]\n",
      "Step #1200 W = -0.8792195\n",
      "Loss = [0.17069075]\n",
      "Step #1400 W = -0.8769954\n",
      "Loss = [0.8258968]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1400):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = [x_vals[rand_index]]\n",
    "    rand_y = [y_vals[rand_index]]\n",
    "    #Run the graph\n",
    "    sess.run(train_step2, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    #Print the result on every 20th iteration\n",
    "    if(i + 1) % 200 == 0:\n",
    "        print('Step #' + str(i+1) + ' W = ' + str(sess.run(w)))\n",
    "        print('Loss = ' + str(sess.run(x_entropy_loss, feed_dict={x_data: rand_x, y_target: rand_y})))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
